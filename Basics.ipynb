{
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language": "lua",
  "language_info": {
   "name": "lua",
   "version": "5.1"
  },
  "name": "",
  "signature": "sha256:f9a40b4f30ba89e8181b1e9f9c7902fa243b4b3c24b9902f1d0964d8ee12f54c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "6.869 Torch Tutorial"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the defining aspects of Torch is that one interfaces with it uses the Lua language.\n",
      "\n",
      "Lua is lightweight and has a [stellar interpreter](http://luajit.org/), that rivals the performance of C and provides easy access to compiled C and CUDA methods. The result is that, instead of defining a computation in a slow language (like Python) and running it in another language (c.f. Theano, TensorFlow), the Lua code that you write is actually what is run! This has a bunch of nifty benefits, the most important of which is debuggability. "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Lua Syntax & Semantics"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Hello, world!') -- shout it from the moon!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "Hello, world!\t\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Presently, we can tell that strings are surrounded by quotes (single or double), functions are called using the `()` operator, and comments are prefixed with `--`.\n",
      "\n",
      "### Variables\n",
      "\n",
      "Like C, Java, and JavaScript, Lua is [lexically scoped](https://en.wikipedia.org/wiki/Scope_%28computer_science%29#Lexical_scope_vs._dynamic_scope), which means that a local variable, defined using the `local` keyword is only bound within the smallest block that contains it. Similarly to JavaScript, a variable defined without using `local` (c.f. `var`) is global and can be accessed anywhere in the program.\n",
      "\n",
      "In Lua, a block, very generally, is any control structure or definition that end with the keyword `end`. The following example demonstrates the `do ... end` block which does nothing other than define a block:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local a = 41\n",
      "do\n",
      "    print('a = '..a)\n",
      "    b = 4\n",
      "    local c = 5\n",
      "end\n",
      "print('b = '..b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "a = 41\t\n",
        "b = 4\t\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Okay, but what haappens if we try to print out `c`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('c = '..c)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "[string \"print('c = '..c)...\"]:1: attempt to concatenate global 'c' (a nil value)\nstack traceback:\n\t[string \"print('c = '..c)...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/a/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/a/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/a/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/a/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b371d20",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "[string \"print('c = '..c)...\"]:1: attempt to concatenate global 'c' (a nil value)\nstack traceback:\n\t[string \"print('c = '..c)...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/a/torch/install/share/lua/5.1/itorch/main.lua:210: in function </Users/a/torch/install/share/lua/5.1/itorch/main.lua:174>\n\t/Users/a/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t/Users/a/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/a/torch/install/share/lua/5.1/itorch/main.lua:389: in main chunk\n\t[C]: in function 'require'\n\t(command line):1: in main chunk\n\t[C]: at 0x010b371d20"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Any undefined variable defaults to `nil`** and, since our local `c` has gone out of scope, the only option is a global variable that hasn't been defined!\n",
      "\n",
      "The main takeaway, here, is that Lua will not complain about undefined variables and--even worse--treat them as `nil`, so if you're not careful, your program might be silently failing!\n",
      "\n",
      "**Note:** a single line/program entered into the iTorch notebook or Torch REPL is implicitly enclosed in a block, so variables have to be defined globally to persist across lines.\n",
      "\n",
      "Now that we have somewhere to store our stuff, let's look at some of the stuff we can store!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Functions\n",
      "\n",
      "It'd be tough to write code that trains function approximators without having some way to define functions, themselves!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "function trainBatch(net, data, labels, learningRate)\n",
      "    local loss = forward(net, data, labels)\n",
      "    backward(net, loss)\n",
      "    updateParameters(net, learningRate)\n",
      "    return loss\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Tables\n",
      "\n",
      "We've saved the best for last: tables. Similarly to JavaScript in which everything is an Object, most things in Lua are tables. If you're coming from Python, you might think of them as a mash-up of a `dict` and a `list` with the extra ability to be used as objects."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = {[0]=0, 1, 2, 3}    -- create a table with an array part holding [1,2,3] and a dict part holding 0=0\n",
      "print('t[1] = '..t[1])  -- Lua uses 1-indexing!\n",
      "print('The contiguous array part of t, starting at 1, has size '..#t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "t[1] = 1\t\n",
        "The contiguous array part of t, starting at 1, has size 3\t\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "-- add some stuff to the dict part\n",
      "t.rng = function() return 4 end\n",
      "print(t.rng())\n",
      "\n",
      "function t.someOtherFn()\n",
      "    print('^ shorthand for defining functions in tables')\n",
      "end\n",
      "\n",
      "t['0'] = 'Z'\n",
      "print(t[0]) -- numbers are not auto-converted! (c.f. JavaScript)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "4\t\n",
        "0\t\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### OOP\n",
      "\n",
      "If you were paying attention, you'll remember that I mentioned that tables are also used for OOP. The syntax here gets a bit weird but if you can get through this part, it's all downhill from here.\n",
      "\n",
      "Really, the only thing that you need to know is that calling a table function (like `t.rng`, above), accessed with `:` instead of `.`, will pass *the table itself* as the first argument, `self`. This is similar to how Python works.\n",
      "\n",
      "Fortunately, Torch makes it easy to define and use classes using [`torch.class`](https://github.com/torch/torch7/blob/master/doc/utility.md#torch.class):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'nn' -- import the neural network package\n",
      "\n",
      "MyModule, parent = torch.class('nn.MyModule', 'nn.Module')\n",
      "\n",
      "function MyModule:__init() -- same as MyModule.__init(self)\n",
      "    parent.__init(self)\n",
      "    self.learning = 'ALL the things!'\n",
      "end\n",
      "\n",
      "function MyModule:updateOutput(input)\n",
      "    print('Learning '..self.learning)\n",
      "end\n",
      "\n",
      "mod = nn.MyModule() -- create a new nn.MyModule\n",
      "mod:updateOutput(torch.rand(3, 3))"
     ],
     "language": "python",
     "metadata": {
      "scrolled": true
     },
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "Learning ALL the things!\t\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Watch out! The thing assigned to `MyModule` line 3 isn't the constructor for your Module: it's actually a [*metatable*](http://www.lua.org/pil/13.html) in which you define the methods of your class. Torch stores the real constructor in a variable named as the first argument to `torch.class`. You can see this in action on line 14.\n",
      "\n",
      "Great, that's about as tough at it gets. Now on to the easy stuff.\n",
      "\n",
      "### Control Flow: Conditionals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "local aBool = 42\n",
      "if aBool == '42' then\n",
      "    print('Yay, strings!')\n",
      "elseif aBool >= 42 then\n",
      "    print('A big number!')\n",
      "else\n",
      "    print(\"I have no idea what's going on!\")\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Control Flow: Loops"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i=3,5 do\n",
      "    print(i)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "3\t\n",
        "4\t\n",
        "5\t\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = {'a', 'b', [7]='you bet!'}\n",
      "for k,v in pairs(t) do\n",
      "    print('t['..k..'] = '..v)\n",
      "end"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "t[1] = a\t\n",
        "t[2] = b\t\n",
        "t[7] = you bet!\t\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tensors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "require 'torch' -- light my fire"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, with some Lua under our belts (space suits?) we can get on to getting on to the fun part of designing and training neural networks! Before we can do that, though, we need to learn an array of things about Torch arrays!\n",
      "\n",
      "If you already use a scientific computing platform like SciPy, Julia, R, or [heaven forbid] MATLAB, you'll probably agree that the n-dimensional array is probably the most imporant feature! Torch is no different. Introducing the `Tensor`:\n",
      "\n",
      "# Tensors\n",
      "\n",
      "The aptly-named [Tensor](https://torch7.readthedocs.io/en/latest/tensor/index.html) is Torch's multi-dimensional array data structure which can be created, indexed, sliced, resized, and otherwise modified to suit all of your mathematical needs. These are distinct from tables and are significantly faster and more compact.\n",
      "\n",
      "There's a caveat, however: a Tensor is just a *view* of some bytes somewhere in memory, or the Tensor's `Storage`. Practically, this means is that operations like slicing, indexing, and reshaping are cheap but allocation and resizing are not! This will be important to keep in mind when working on the GPU as allocations are slow and can leak memory.\n",
      "\n",
      "## Creating Tensors\n",
      "\n",
      "The simplest and most generic way of getting a Tensor is to use `torch.Tensor()`. Passing in sizes will automatically allocate (but not initialize!) a `Storage`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "torch.Tensor(2, 3, 4)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you're so inclined, you can create a Tensor filled with zeros or ones by using torch.zeros() and torch.ones(), respectively. Additionally, you can pass in your own data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "torch.Tensor{{1, 2, 3}, {4, 5, 6}, {7, 8, 9}}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tensor types\n",
      "\n",
      "\"Wait a second\" you say. \"What's this bit about `torch.DoubleTensor`?\"\n",
      "\n",
      "Another great question! That's the *type* of the Tensor which determines how the rest of Torch interprets the data contained by the `Storage`. Here are some more of the Tensor types that you should know:\n",
      "\n",
      "* `torch.FloatTensor`: 32-bit floating-point (useful for data sent to/received from a 32-bit floating-point GPU)\n",
      "* `torch.LongTensor`: 64-bit integers (useful for holding array indices)\n",
      "* `torch.ByteTensor`: 8-bit unsigned numbers (useful for holding binary masks)\n",
      "* `torch.CudaTensor`: 32-bit fp numbers that have their storage allocated on the GPU\n",
      "* `torch.CudaHalfTensor`: new-fangled type that uses half the memory of fp32 and is faster on some GPUs (e.g., Pascal)\n",
      "\n",
      "Don't concern yourself too much, yet, with the fancy CUDA types. Since we don't have GPUs for this workshop, we won't need them, but they're virtually indispensable when training larger, more complicated models. Figuring out how the CUDA backend works has a slight (read: quite steep) learning curve, so there'll be a guide on this later.\n",
      "\n",
      "The main idea is that certain operations require certain Tensor types; you always convert a Tensor and its Storage to a different type (allocation warning!) by calling `theTensor:<type>()`"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(torch.rand(2, 3):mul(42):long()) -- converts using floor(x)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## It's My Data and I Want it NOW! (View A.G. Tensworth 877...)\n",
      "\n",
      "When we're not crashing dense matrices in multiplications, we usually only want to deal with small segments of our data. For this, all we must do is create new Tensors that only look at a portion of the original Storage.\n",
      "\n",
      "To help introduce the next concepts, we'll need a volunteer:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "friend = torch.Tensor(3, 4):range(1, 12)\n",
      "print(friend, friend:size())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Slicing and Indexing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "row1 = friend[3]\n",
      "col2 = friend[{{}, 2}]\n",
      "eight = friend[{2, 4}]\n",
      "sevenEleven = friend[{{2,3}, 3}]\n",
      "print(sevenEleven)\n",
      "assert(friend:size(1) == 3 and friend:size(2) == 4) -- friend remains unchanged"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Assignment\n",
      "\n",
      "Not bad! Now that we can tell Torch which indices we want, we can start to get choosy with their contents.\n",
      "\n",
      "When using the convenient table syntax, the `=` operator is overloaded to do assignment. If, instead, you go the route of `select`, `narrow`, and `sub`, you'll have to use the likes of `copy` and `fill`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "oldFriend = friend:clone() -- forgotten but not gone\n",
      "friend[{1, 1}] = 0\n",
      "print(friend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "friend:select(1, 3):copy(oldFriend[1]):mul(-1)\n",
      "print(friend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "friend:narrow(2, 2, 2):select(1, 2):zero()\n",
      "print(friend)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Presented above was only a selected subset (no pun intended) of the full Tensor indexing API. You will want to familiarize yourself with [the rest of the commands](https://torch7.readthedocs.io/en/latest/tensor/index.html) in order to write maximally terse and performant code!\n",
      "\n",
      "## Tensor Lifecycle\n",
      "\n",
      "When training neural networks on the GPU, the life of a Tensor looks a bit like this\n",
      "\n",
      "1. Create a Tensor in RAM\n",
      "2. Fill the Tensor with a batch of data\n",
      "3. Copy the Tensor to a pre-allocated counterpart on the GPU\n",
      "4. Trash the RAM Tensor\n",
      "\n",
      "It's kind of like a bucket brigade except with numbers for water, GPUs for burning buildings, and math for fire!\n",
      "\"Gee, I can't imagine how lit this guy must have been when he came up with that one...\" you think to yourself.  \n",
      "Well, let's just say that I have a few models training while I'm writing this.  \n",
      "I bet you're really confused now!"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}