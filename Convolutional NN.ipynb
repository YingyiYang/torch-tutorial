{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution\n",
    "Up until now, we've been treating an MNIST digit as a really, really big vector. This is odd because, as far as the network knows, each component/pixel is independent of the rest. Since the input is actually a 2D image, we know that this isn't the case! How, then, can we take advantage of this additional structure?\n",
    "\n",
    "If we wanted to stick with linear layers, we could try connecting each $3\\times 3$ neighborhood of pixels to a single output. In that case, if we had a $5\\times 5$ image, we'd have four separate `nn.Linear` modules producing a $3\\times 3$ output. That's not too bad except it assumes that the information learned about the top left neighborhood is independent from that learned about the bottom right. For instance, if we wanted to detect a 1, we'd need a vertical line detector; a repeated-linear network would have to learn this same detector for every position in the image!\n",
    "\n",
    "Much better would be to, instead, learn a bunch of different $n\\times n$ *filters* to slide (or \"*convolve*\") over each $n\\times n$ neighborhood in the image. Now, you can have one filter that detects horizontal edges, another one that detects vertical edges, and possibly even one that detects the holes inside of 0s and 9s! The best part is that now you only have to learn $n^2 \\cdot numFilters$ parameters instead of $n^2 \\cdot (N-2)^2$, which speeds up computation and helps stave off overfitting.\n",
    "\n",
    "If this doesn't make sense yet, don't fret. First, check out this diagram of the computation performed during an application of a convolutional filter.\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/GvsBA.jpg\">\n",
    "<div class=\"figcaption\">Figure 1: Computation performed during an application of a single conv filter.</div>\n",
    "\n",
    "Still considering the image in Figure 1, to produce the entire *output feature map*, one must simply apply the conv filter at every valid location in the image. Note that, due to edge effects, the output feature map has size $o = N - n + 1$ (where $N$ is the input size and $n$ is the kernel/filter size).\n",
    "\n",
    "If you grok the previous bit, this next part should be a simple extension: if you have $f$ filters, applying all of them to an input will give you $f$ output feature maps. You can then stack those up into a *volume* of size $f\\times o \\times o$ and feed them into the next conv layer. This is to say that, in general, the input to a single conv filter is an $f \\times n \\times n$ volume. In fact, you can think of an RGB image as a special case of a conv volume where each channel is one feature map (albeit one that is highly correlated with the others). \n",
    "\n",
    "Again, for you diagram people:\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png\" style=\"width: 33%\">\n",
    "<div class=\"figcaption\">Figure 2: Input and output conv volumes.</div>\n",
    "\n",
    "As each feature map goes through more and more convolutional layers, the features become increasingly abstract. For example, your first layer might contain a horizontal and vertical edge detector. In the second layer, a filer could then combine the feature maps of the - and | detectors to yield a + detector! \n",
    "\n",
    "If diagrams aren't your style and you prefer the cold, hard code the module of the hour is [`nn.SpatialConvolution`](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialConvolution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.SpatialConvolution(3 -> 5, 3x3)\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'nn'\n",
    "conv = nn.SpatialConvolution(3, 5, 3, 3) -- 1 feature map of size 28x28 -> 5 feature maps of size 26x26\n",
    "print(tostring(conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  2\n",
       "  5\n",
       " 26\n",
       " 26\n",
       "[torch.LongStorage of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(2, 3, 28, 28) -- perhaps 2 MNIST images\n",
    "conv:forward(img)\n",
    "print(conv.output:size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Taking a Page from Term Papers\n",
    "\n",
    "One last thing before we slide on to the next part: if you want to capture details present at the edges of the image, the general consensus is that one should add a border of zeros known as *padding*. So, if we wanted to produce a $28 \\times 28$ output in the previous example, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  2\n",
       "  5\n",
       " 28\n",
       " 28\n",
       "[torch.LongStorage of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padW, padH = 1, 1\n",
    "conv = nn.SpatialConvolution(3, 5, 3, 3, 1, 1, padW, padH) -- add a frame of width 1 before convolving\n",
    "print(conv:forward(img):size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Pooling, in a nutshell, throws out several pixels in a neighborhood in exchange for *translation invariance*, which is just a fancy way of saying that shifting your input won't significantly change your output. \n",
    "\n",
    "The two most common types of pooling are max and average pooling, both of which are available in Torch as [`nn.SpatalMaxPooling`](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialMaxPooling) and [`nn.SpatialAveragePooling`](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialAveragePooling).\n",
    "\n",
    "Is it that time already for more diagrams?\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png\" style=\"width: 33%\">\n",
    "<div class=\"figcaption\">Figure 3: Max pooling of a single feature map using a $2\\times 2$ kernel and stride.</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.SpatialMaxPooling(2x2, 2,2)\t\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = nn.SpatialMaxPooling(2, 2) -- takes the max in a 2x2 neighborhood\n",
    "print(tostring(pool))             -- slides by 2 px after each application (there's no overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  17  40  93  77  19  60\n",
       "  23  36  59  55  43  74\n",
       "  20  55  74  62  53  64\n",
       "  86  63  25  63  51  36\n",
       "  66  68  17  67  88  17\n",
       "  80   8  41  60  51  79\n",
       "[torch.DoubleTensor of size 1x6x6]\n",
       "\n",
       "(1,.,.) = \n",
       "  40  93  74\n",
       "  86  74  64\n",
       "  80  67  88\n",
       "[torch.DoubleTensor of size 1x3x3]\n",
       "\n",
       "(1,.,.) = \n",
       "  0  1  1  0  0  0\n",
       "  0  0  0  0  0  1\n",
       "  0  0  1  0  0  1\n",
       "  1  0  0  0  0  0\n",
       "  0  0  0  1  1  0\n",
       "  1  0  0  0  0  0\n",
       "[torch.DoubleTensor of size 1x6x6]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.Tensor(1, 6, 6):random(99)\n",
    "print(img)\n",
    "print(pool:forward(img))\n",
    "print(pool:backward(img, torch.ones(1, 3, 3))) -- mask of maximal pixels in each neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Strided convolutions\n",
    "\n",
    "Torch lets you do strided convolutions using the `dW` and `dH` parameters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.SpatialConvolution(3 -> 5, 3x3, 2,2)\t\n",
       "  2\n",
       "  5\n",
       " 13\n",
       " 13\n",
       "[torch.LongStorage of size 4]\n",
       "\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(2, 3, 28, 28)\n",
    "dW, dH = 2, 2\n",
    "conv = nn.SpatialConvolution(3, 5, 3, 3, dW, dH) -- stride of 2 in each direction\n",
    "print(tostring(conv))\n",
    "print(conv:forward(img):size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To 99.7% and Beyond!\n",
    "\n",
    "With convolutions and pooling in hand, we can now go about improving our paltry 98% on MNIST to something that's actually acceptable.\n",
    "\n",
    "We'll use the same training framework as before; the only thing that will change is the model, which you'll place in [`mnist/models/conv.lua`](../edit/mnist/models/conv.lua).\n",
    "\n",
    "Whereas before we stacked some combination of `nn.Linear` and `nn.ReLU`, this time around, you'll want to do something more akin to [`nn.SpatialConvolution`](https://github.com/torch/nn/blob/master/SpatialConvolution.lua), [`nn.ReLU`](https://github.com/torch/nn/blob/master/doc/transfer.md#relu), and then throw in a [`nn.SpatialMaxPooling`](https://github.com/torch/nn/blob/master/doc/convolution.md#nn.SpatialMaxPooling) every so often.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nn.Sequential {\n",
       "  [input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> output]\n",
       "  (1): nn.SpatialConvolutionMM(1 -> 32, 5x5)\n",
       "  (2): nn.Tanh\n",
       "  (3): nn.SpatialMaxPooling(3x3, 3,3, 1,1)\n",
       "  (4): nn.SpatialConvolutionMM(32 -> 64, 5x5)\n",
       "  (5): nn.Tanh\n",
       "  (6): nn.SpatialMaxPooling(2x2, 2,2)\n",
       "  (7): nn.Reshape(256)\n",
       "  (8): nn.Linear(256 -> 200)\n",
       "  (9): nn.Tanh\n",
       "  (10): nn.Linear(200 -> 10)\n",
       "}\n",
       "{\n",
       "  gradInput : DoubleTensor - empty\n",
       "  modules : \n",
       "    {\n",
       "      1 : \n",
       "        nn.SpatialConvolutionMM(1 -> 32, 5x5)\n",
       "        {\n",
       "          dH : 1\n",
       "          dW : 1\n",
       "          nInputPlane : 1\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 5\n",
       "          gradBias : DoubleTensor - size: 32\n",
       "          padH : 0\n",
       "          bias : DoubleTensor - size: 32\n",
       "          weight : DoubleTensor - size: 32x25\n",
       "          _type : torch.DoubleTensor\n",
       "          gradWeight : DoubleTensor - size: 32x25\n",
       "          padW : 0\n",
       "          nOutputPlane : 32\n",
       "          kW : 5\n",
       "          gradInput : DoubleTensor - empty\n",
       "        }\n",
       "      2 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      3 : \n",
       "        nn.SpatialMaxPooling(3x3, 3,3, 1,1)\n",
       "        {\n",
       "          dH : 3\n",
       "          dW : 3\n",
       "          kW : 3\n",
       "          gradInput : DoubleTensor - empty\n",
       "          indices : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          padH : 1\n",
       "          ceil_mode : false\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 3\n",
       "          padW : 1\n",
       "        }\n",
       "  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "    4 : \n",
       "        nn.SpatialConvolutionMM(32 -> 64, 5x5)\n",
       "        {\n",
       "          dH : 1\n",
       "          dW : 1\n",
       "          nInputPlane : 32\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 5\n",
       "          gradBias : DoubleTensor - size: 64\n",
       "          padH : 0\n",
       "          bias : DoubleTensor - size: 64\n",
       "          weight : DoubleTensor - size: 64x800\n",
       "          _type : torch.DoubleTensor\n",
       "          gradWeight : DoubleTensor - size: 64x800\n",
       "          padW : 0\n",
       "          nOutputPlane : 64\n",
       "          kW : 5\n",
       "          gradInput : DoubleTensor - empty\n",
       "        }\n",
       "      5 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      6 : \n",
       "        nn.SpatialMaxPooling(2x2, 2,2)\n",
       "        {\n",
       "          dH : 2\n",
       "          dW : 2\n",
       "          kW : 2\n",
       "          gradInput : DoubleTensor - empty\n",
       "          indices : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          padH : 0\n",
       "          ceil_mode : false\n",
       "          output : DoubleTensor - empty\n",
       "          kH : 2\n",
       "          padW : 0\n",
       "        }\n",
       "      7 : \n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "        nn.Reshape(256)\n",
       "        {\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          size : LongStorage - size: 1\n",
       "          nelement : 256\n",
       "          batchsize : LongStorage - size: 2\n",
       "        }\n",
       "      8 : \n",
       "        nn.Linear(256 -> 200)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 200\n",
       "          weight : DoubleTensor - size: 200x256\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 200\n",
       "          gradWeight : DoubleTensor - size: 200x256\n",
       "        }\n",
       "      9 : \n",
       "        nn.Tanh\n",
       "        {\n",
       "          gradInput : DoubleTensor - empty\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "        }\n",
       "      10 : \n",
       "        nn.Linear(200 -> 10)\n",
       "        {\n",
       "          gradBias : DoubleTensor - size: 10\n",
       "          weight : DoubleTensor - size: 10x200\n",
       "          _type : torch.DoubleTensor\n",
       "          output : DoubleTensor - empty\n",
       "          gradInput : DoubleTensor - empty\n",
       "          bias : DoubleTensor - size: 10\n",
       "          gradWeight : DoubleTensor - size: 10x200\n",
       "        }\n",
       "    }\n",
       "  _type : torch.DoubleTensor\n",
       "  output : DoubleTensor - empty\n",
       "}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local model = nn.Sequential()\n",
    "      model:add(nn.SpatialConvolutionMM(1, 32, 5, 5))\n",
    "      model:add(nn.Tanh())\n",
    "      model:add(nn.SpatialMaxPooling(3, 3, 3, 3, 1, 1))\n",
    "      -- stage 2 : mean suppresion -> filter bank -> squashing -> max pooling\n",
    "      model:add(nn.SpatialConvolutionMM(32, 64, 5, 5))\n",
    "      model:add(nn.Tanh())\n",
    "      model:add(nn.SpatialMaxPooling(2, 2, 2, 2))\n",
    "      -- stage 3 : standard 2-layer MLP:\n",
    "      model:add(nn.Reshape(64*2*2))\n",
    "      model:add(nn.Linear(64*2*2, 200))\n",
    "      model:add(nn.Tanh())\n",
    "      model:add(nn.Linear(200, 10))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, verify that your network has the correct input/output sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Passed!\t\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dofile('mnist/test/conv_io.lua') -- check the Tensor sizes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train to your heart's content! (Don't forget to twiddle hyperparameters!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 4s384ms | Step: 67ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch  1 | train loss: 1.323 | val loss: 0.525 | val acc: 89.76%\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 4s79ms | Step: 62ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch  2 | train loss: 0.367 | val loss: 0.250 | val acc: 94.01%\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 4s223ms | Step: 64ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch  3 | train loss: 0.220 | val loss: 0.175 | val acc: 95.64%\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 4s50ms | Step: 62ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch  4 | train loss: 0.164 | val loss: 0.136 | val acc: 96.46%\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [======================================================>]                               \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b Tot: 4s44ms | Step: 62ms\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch  5 | train loss: 0.134 | val loss: 0.115 | val acc: 96.96%\t\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMNIST = dofile('mnist/main.lua')\n",
    "trainMNIST({modelType='conv', nEpochs=5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language": "lua",
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
